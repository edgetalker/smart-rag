#!/usr/bin/env python3
"""
Smart RAG æ•°æ®é›†æ„å»ºå™¨
ä½¿ç”¨Git Cloneæ–¹å¼è·å–å¼€æºé¡¹ç›®çš„Markdownæ–‡æ¡£
"""

import os
import subprocess
import shutil
import yaml
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import logging
from tqdm import tqdm
import time
import json

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('dataset_builder.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class SmartRAGDatasetBuilder:
    """Smart RAGé¡¹ç›®æ•°æ®é›†æ„å»ºå™¨"""

    def __init__(self, config_path: str = "configs/data_sources.yaml"):
        """åˆå§‹åŒ–æ•°æ®é›†æ„å»ºå™¨"""
        self.config_path = Path(config_path)
        self.config = self._load_config()

        # è®¾ç½®å·¥ä½œç›®å½•
        self.raw_data_dir = Path("raw_data")
        self.processed_data_dir = Path("processed_data")
        self.documents_dir = self.processed_data_dir / "documents"
        self.metadata_dir = self.processed_data_dir / "metadata"
        self.reports_dir = self.processed_data_dir / "quality_reports"

        # åˆ›å»ºç›®å½•ç»“æ„
        self._setup_directories()

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_repos_cloned": 0,
            "total_files_found": 0,
            "total_files_processed": 0,
            "total_files_valid": 0,
            "processing_errors": 0,
            "start_time": None,
            "end_time": None
        }

    def _load_config(self) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            logger.error(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            raise

    def _setup_directories(self):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•ç»“æ„"""
        directories = [
            self.raw_data_dir,
            self.processed_data_dir,
            self.documents_dir,
            self.metadata_dir,
            self.reports_dir
        ]

        for directory in directories:
            directory.mkdir(parents=True, exist_ok=True)
            logger.info(f"ğŸ“ åˆ›å»ºç›®å½•: {directory}")

    def clone_repository(self, source_name: str, source_config: Dict) -> Path:
        """å…‹éš†Gitä»“åº“"""
        repo_url = source_config["repo_url"]
        repo_dir = self.raw_data_dir / f"{source_name}_repo"

        logger.info(f"ğŸ”„ å¼€å§‹å…‹éš† {source_name} ä»“åº“...")
        logger.info(f"ğŸ“ URL: {repo_url}")

        # å¦‚æœç›®å½•å·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤
        if repo_dir.exists():
            logger.warning(f"ç›®å½•å·²å­˜åœ¨ï¼Œåˆ é™¤æ—§ç‰ˆæœ¬: {repo_dir}")
            shutil.rmtree(repo_dir)

        try:
            # ä½¿ç”¨shallow cloneæé«˜é€Ÿåº¦
            cmd = [
                "git", "clone",
                "--depth", "1",  # åªå…‹éš†æœ€æ–°ç‰ˆæœ¬
                "--single-branch",  # åªå…‹éš†é»˜è®¤åˆ†æ”¯
                repo_url,
                str(repo_dir)
            ]

            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
            )

            if result.returncode != 0:
                raise Exception(f"Git cloneå¤±è´¥: {result.stderr}")

            logger.info(f"âœ… {source_name} å…‹éš†æˆåŠŸ")
            self.stats["total_repos_cloned"] += 1
            return repo_dir

        except subprocess.TimeoutExpired:
            logger.error(f"âŒ {source_name} å…‹éš†è¶…æ—¶")
            raise
        except Exception as e:
            logger.error(f"âŒ {source_name} å…‹éš†å¤±è´¥: {e}")
            raise

    def extract_documentation(self, source_name: str, repo_dir: Path, source_config: Dict) -> List[Path]:
        """ä»ä»“åº“ä¸­æå–æ–‡æ¡£æ–‡ä»¶"""
        docs_path = source_config["docs_path"]
        file_patterns = source_config.get("file_patterns", ["*.md"])
        exclude_patterns = source_config.get("exclude_patterns", [])

        docs_dir = repo_dir / docs_path

        if not docs_dir.exists():
            logger.warning(f"âš ï¸  æ–‡æ¡£ç›®å½•ä¸å­˜åœ¨: {docs_dir}")
            return []

        logger.info(f"ğŸ“‚ æå– {source_name} æ–‡æ¡£ï¼Œç›®å½•: {docs_dir}")

        # æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„æ–‡ä»¶
        all_files = []
        for pattern in file_patterns:
            found_files = list(docs_dir.rglob(pattern))
            all_files.extend(found_files)

        # è¿‡æ»¤æ’é™¤çš„æ–‡ä»¶
        filtered_files = []
        for file_path in all_files:
            should_exclude = False
            for exclude_pattern in exclude_patterns:
                if exclude_pattern.replace("**", "*") in str(file_path):
                    should_exclude = True
                    break

            if not should_exclude:
                filtered_files.append(file_path)

        logger.info(f"ğŸ“Š {source_name} æ‰¾åˆ°æ–‡æ¡£æ–‡ä»¶: {len(filtered_files)} ä¸ª")
        self.stats["total_files_found"] += len(filtered_files)

        return filtered_files

    def process_markdown_file(self, file_path: Path, source_name: str) -> Optional[Dict]:
        """å¤„ç†å•ä¸ªMarkdownæ–‡ä»¶"""
        try:
            # è¯»å–æ–‡ä»¶å†…å®¹
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()

            # åŸºç¡€éªŒè¯
            if len(content.strip()) < self.config["processing"]["min_content_length"]:
                logger.debug(f"æ–‡ä»¶å†…å®¹è¿‡çŸ­ï¼Œè·³è¿‡: {file_path}")
                return None

            if len(content) > self.config["processing"]["max_content_length"]:
                logger.debug(f"æ–‡ä»¶å†…å®¹è¿‡é•¿ï¼Œè·³è¿‡: {file_path}")
                return None

            # æå–æ–‡æ¡£ä¿¡æ¯
            document_info = self._extract_document_info(content, file_path, source_name)

            self.stats["total_files_processed"] += 1
            return document_info

        except Exception as e:
            logger.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            self.stats["processing_errors"] += 1
            return None

    def _extract_document_info(self, content: str, file_path: Path, source_name: str) -> Dict:
        """æå–æ–‡æ¡£çš„è¯¦ç»†ä¿¡æ¯"""
        import frontmatter
        import re

        # è§£æfrontmatter
        try:
            post = frontmatter.loads(content)
            metadata = post.metadata
            main_content = post.content
        except:
            metadata = {}
            main_content = content

        # æå–æ ‡é¢˜ç»“æ„
        headings = self._extract_headings(main_content)

        # æå–ä»£ç å—
        code_blocks = self._extract_code_blocks(main_content)

        # æå–é“¾æ¥
        links = self._extract_links(main_content)

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        stats = self._calculate_content_stats(main_content, code_blocks)

        # ç”Ÿæˆç›¸å¯¹è·¯å¾„ï¼ˆç”¨ä½œæ–‡æ¡£IDï¼‰
        relative_path = str(file_path).replace(str(self.raw_data_dir), "")

        document_info = {
            "id": f"{source_name}_{hash(relative_path) % 1000000}",
            "source": source_name,
            "file_path": str(file_path),
            "relative_path": relative_path,
            "title": self._extract_title(headings, metadata),
            "metadata": metadata,
            "content": main_content,
            "headings": headings,
            "code_blocks": code_blocks,
            "links": links,
            "stats": stats,
            "created_at": time.time()
        }

        return document_info

    def _extract_headings(self, content: str) -> List[Dict]:
        """æå–æ ‡é¢˜ç»“æ„"""
        import re

        heading_pattern = r'^(#{1,6})\s+(.+)$'
        headings = []

        for i, line in enumerate(content.split('\n')):
            match = re.match(heading_pattern, line.strip())
            if match:
                level = len(match.group(1))
                text = match.group(2).strip()
                headings.append({
                    "level": level,
                    "text": text,
                    "line_number": i + 1
                })

        return headings

    def _extract_code_blocks(self, content: str) -> List[Dict]:
        """æå–ä»£ç å—"""
        import re

        # åŒ¹é…ä»£ç å— ```language ... ```
        code_pattern = r'```(\w+)?\n(.*?)\n```'
        code_blocks = []

        for match in re.finditer(code_pattern, content, re.DOTALL):
            language = match.group(1) or "text"
            code = match.group(2).strip()

            code_blocks.append({
                "language": language,
                "code": code,
                "start_pos": match.start(),
                "end_pos": match.end()
            })

        return code_blocks

    def _extract_links(self, content: str) -> List[Dict]:
        """æå–é“¾æ¥"""
        import re

        # åŒ¹é…Markdowné“¾æ¥ [text](url)
        link_pattern = r'\[([^\]]+)\]\(([^)]+)\)'
        links = []

        for match in re.finditer(link_pattern, content):
            text = match.group(1)
            url = match.group(2)

            links.append({
                "text": text,
                "url": url,
                "is_internal": not url.startswith(("http://", "https://"))
            })

        return links

    def _calculate_content_stats(self, content: str, code_blocks: List[Dict]) -> Dict:
        """è®¡ç®—å†…å®¹ç»Ÿè®¡ä¿¡æ¯"""
        lines = content.split('\n')

        # è®¡ç®—ä»£ç è¡Œæ•°
        code_lines = sum(len(block["code"].split('\n')) for block in code_blocks)

        # è®¡ç®—æ–‡æœ¬è¡Œæ•°ï¼ˆéç©ºè¡Œï¼‰
        text_lines = len([line for line in lines if line.strip()])

        stats = {
            "total_lines": len(lines),
            "text_lines": text_lines,
            "code_lines": code_lines,
            "code_blocks_count": len(code_blocks),
            "character_count": len(content),
            "word_count": len(content.split()),
            "code_ratio": code_lines / max(text_lines, 1)
        }

        return stats

    def _safe_cleanup(self, directory: Path):
        """å®‰å…¨æ¸…ç†ç›®å½•ï¼ˆè§£å†³Windowsæƒé™é—®é¢˜ï¼‰"""
        import stat

        def handle_remove_readonly(func, path, exc):
            """å¤„ç†åªè¯»æ–‡ä»¶çš„åˆ é™¤"""
            if os.path.exists(path):
                # ç§»é™¤åªè¯»å±æ€§
                os.chmod(path, stat.S_IWRITE)
                # é‡æ–°å°è¯•åˆ é™¤
                func(path)

        try:
            if directory.exists():
                # Windowsç‰¹æ®Šå¤„ç†ï¼šç§»é™¤åªè¯»å±æ€§
                if os.name == 'nt':  # Windowsç³»ç»Ÿ
                    for root, dirs, files in os.walk(directory):
                        for dir_name in dirs:
                            dir_path = os.path.join(root, dir_name)
                            try:
                                os.chmod(dir_path, stat.S_IWRITE)
                            except:
                                pass
                        for file_name in files:
                            file_path = os.path.join(root, file_name)
                            try:
                                os.chmod(file_path, stat.S_IWRITE)
                            except:
                                pass

                    # ä½¿ç”¨shutil.rmtreeçš„onerrorå›è°ƒå¤„ç†å‰©ä½™çš„æƒé™é—®é¢˜
                    shutil.rmtree(directory, onerror=handle_remove_readonly)
                else:
                    # Linux/Macä½¿ç”¨æ ‡å‡†åˆ é™¤
                    shutil.rmtree(directory)

                logger.info(f"âœ… æˆåŠŸæ¸…ç†ç›®å½•: {directory}")
        except Exception as e:
            logger.warning(f"âš ï¸  æ¸…ç†ç›®å½•å¤±è´¥ {directory}: {e}")
            logger.warning("ğŸ’¡ æ¸…ç†å¤±è´¥ä¸å½±å“æ•°æ®å¤„ç†ç»“æœï¼Œå¯æ‰‹åŠ¨åˆ é™¤raw_dataç›®å½•")

    def _extract_title(self, headings: List[Dict], metadata: Dict) -> str:
        """æå–æ–‡æ¡£æ ‡é¢˜"""
        # ä¼˜å…ˆä½¿ç”¨metadataä¸­çš„title
        if "title" in metadata:
            return metadata["title"]

        # ä½¿ç”¨ç¬¬ä¸€ä¸ªä¸€çº§æ ‡é¢˜
        for heading in headings:
            if heading["level"] == 1:
                return heading["text"]

        # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ ‡é¢˜
        if headings:
            return headings[0]["text"]

        return "Untitled"

    def save_processed_documents(self, documents: List[Dict], source_name: str):
        """ä¿å­˜å¤„ç†åçš„æ–‡æ¡£"""

        # ä¿å­˜å®Œæ•´æ–‡æ¡£æ•°æ®
        documents_file = self.documents_dir / f"{source_name}_documents.json"
        with open(documents_file, 'w', encoding='utf-8') as f:
            json.dump(documents, f, ensure_ascii=False, indent=2)

        # ä¿å­˜å…ƒæ•°æ®æ‘˜è¦
        metadata_summary = []
        for doc in documents:
            summary = {
                "id": doc["id"],
                "title": doc["title"],
                "file_path": doc["relative_path"],
                "stats": doc["stats"],
                "headings_count": len(doc["headings"]),
                "code_blocks_count": len(doc["code_blocks"]),
                "links_count": len(doc["links"])
            }
            metadata_summary.append(summary)

        metadata_file = self.metadata_dir / f"{source_name}_metadata.json"
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata_summary, f, ensure_ascii=False, indent=2)

        logger.info(f"ğŸ’¾ {source_name} æ–‡æ¡£å·²ä¿å­˜: {len(documents)} ä¸ª")

    def process_data_source(self, source_name: str) -> List[Dict]:
        """å¤„ç†å•ä¸ªæ•°æ®æº"""
        source_config = self.config["data_sources"][source_name]

        logger.info(f"ğŸš€ å¼€å§‹å¤„ç†æ•°æ®æº: {source_name}")
        logger.info(f"ğŸ“ æè¿°: {source_config['description']}")

        try:
            # 1. å…‹éš†ä»“åº“
            repo_dir = self.clone_repository(source_name, source_config)

            # 2. æå–æ–‡æ¡£æ–‡ä»¶
            doc_files = self.extract_documentation(source_name, repo_dir, source_config)

            if not doc_files:
                logger.warning(f"âš ï¸  {source_name} æœªæ‰¾åˆ°æœ‰æ•ˆæ–‡æ¡£æ–‡ä»¶")
                self._safe_cleanup(repo_dir)
                return []

            # 3. å¤„ç†æ–‡æ¡£æ–‡ä»¶
            documents = []
            logger.info(f"ğŸ“„ å¼€å§‹å¤„ç† {len(doc_files)} ä¸ªæ–‡æ¡£æ–‡ä»¶...")

            for file_path in tqdm(doc_files, desc=f"å¤„ç†{source_name}æ–‡æ¡£"):
                doc_info = self.process_markdown_file(file_path, source_name)
                if doc_info:
                    documents.append(doc_info)
                    self.stats["total_files_valid"] += 1

            # 4. ä¿å­˜å¤„ç†ç»“æœ
            if documents:
                self.save_processed_documents(documents, source_name)

            # 5. å®‰å…¨æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            logger.info(f"ğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {repo_dir}")
            self._safe_cleanup(repo_dir)

            logger.info(f"âœ… {source_name} å¤„ç†å®Œæˆï¼Œæœ‰æ•ˆæ–‡æ¡£: {len(documents)} ä¸ª")
            return documents

        except Exception as e:
            logger.error(f"âŒ {source_name} å¤„ç†å¤±è´¥: {e}")
            # ç¡®ä¿å³ä½¿å‡ºé”™ä¹Ÿè¦æ¸…ç†
            if 'repo_dir' in locals():
                self._safe_cleanup(repo_dir)
            return []

    def build_complete_dataset(self) -> Dict:
        """æ„å»ºå®Œæ•´æ•°æ®é›†"""
        self.stats["start_time"] = time.time()

        logger.info("ğŸ¯ å¼€å§‹æ„å»ºSmart RAGæ•°æ®é›†")
        logger.info("=" * 60)

        all_documents = []
        source_summary = {}

        # æŒ‰ä¼˜å…ˆçº§æ’åºå¤„ç†æ•°æ®æº
        sources = sorted(
            self.config["data_sources"].items(),
            key=lambda x: x[1]["priority"]
        )

        for source_name, source_config in sources:
            documents = self.process_data_source(source_name)

            # è¯¦ç»†è®°å½•æ¯ä¸ªæ•°æ®æºçš„ç»“æœ
            logger.info(f"ğŸ“Š {source_name} è¿”å›æ–‡æ¡£æ•°é‡: {len(documents)}")

            if documents:
                all_documents.extend(documents)
                logger.info(f"âœ… {source_name} æ–‡æ¡£å·²æ·»åŠ åˆ°å®Œæ•´æ•°æ®é›†")
            else:
                logger.warning(f"âš ï¸  {source_name} æ²¡æœ‰è¿”å›æœ‰æ•ˆæ–‡æ¡£")

            source_summary[source_name] = {
                "document_count": len(documents),
                "description": source_config["description"],
                "priority": source_config["priority"]
            }

        # éªŒè¯å®Œæ•´æ•°æ®é›†
        logger.info(f"ğŸ“Š å®Œæ•´æ•°æ®é›†æ–‡æ¡£æ€»æ•°: {len(all_documents)}")

        if not all_documents:
            logger.error("âŒ å®Œæ•´æ•°æ®é›†ä¸ºç©ºï¼è¯·æ£€æŸ¥å„æ•°æ®æºçš„å¤„ç†ç»“æœ")
            # å°è¯•ä»å•ç‹¬çš„æ–‡ä»¶åŠ è½½æ•°æ®
            all_documents = self._recover_from_individual_files()

        # ä¿å­˜å®Œæ•´æ•°æ®é›†
        complete_dataset_file = self.processed_data_dir / "complete_dataset.json"
        logger.info(f"ğŸ’¾ ä¿å­˜å®Œæ•´æ•°æ®é›†åˆ°: {complete_dataset_file}")

        try:
            with open(complete_dataset_file, 'w', encoding='utf-8') as f:
                json.dump(all_documents, f, ensure_ascii=False, indent=2)

            # éªŒè¯ä¿å­˜ç»“æœ
            file_size = complete_dataset_file.stat().st_size / (1024 * 1024)  # MB
            logger.info(f"âœ… å®Œæ•´æ•°æ®é›†ä¿å­˜æˆåŠŸï¼Œæ–‡ä»¶å¤§å°: {file_size:.2f}MB")

        except Exception as e:
            logger.error(f"âŒ ä¿å­˜å®Œæ•´æ•°æ®é›†å¤±è´¥: {e}")
            return {"error": "Failed to save complete dataset"}

        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        self.stats["end_time"] = time.time()
        final_report = self._generate_final_report(source_summary)

        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        final_report["actual_documents_saved"] = len(all_documents)

        # ä¿å­˜æŠ¥å‘Š
        report_file = self.reports_dir / "build_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(final_report, f, ensure_ascii=False, indent=2)

        logger.info("ğŸ‰ æ•°æ®é›†æ„å»ºå®Œæˆï¼")
        logger.info("=" * 60)
        self._print_final_summary(final_report)

        return final_report

    def _recover_from_individual_files(self) -> List[Dict]:
        """ä»å•ç‹¬çš„æ•°æ®æºæ–‡ä»¶æ¢å¤å®Œæ•´æ•°æ®é›†"""
        logger.info("ğŸ”„ å°è¯•ä»å•ç‹¬æ–‡ä»¶æ¢å¤æ•°æ®é›†...")

        recovered_documents = []
        documents_dir = self.documents_dir

        if not documents_dir.exists():
            logger.error(f"âŒ æ–‡æ¡£ç›®å½•ä¸å­˜åœ¨: {documents_dir}")
            return []

        # æŸ¥æ‰¾æ‰€æœ‰çš„*_documents.jsonæ–‡ä»¶
        for json_file in documents_dir.glob("*_documents.json"):
            logger.info(f"ğŸ“„ å°è¯•åŠ è½½: {json_file}")

            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    documents = json.load(f)

                if isinstance(documents, list):
                    recovered_documents.extend(documents)
                    logger.info(f"âœ… ä» {json_file.name} æ¢å¤äº† {len(documents)} ä¸ªæ–‡æ¡£")
                else:
                    logger.warning(f"âš ï¸  {json_file.name} æ ¼å¼ä¸æ­£ç¡®")

            except Exception as e:
                logger.error(f"âŒ æ— æ³•åŠ è½½ {json_file}: {e}")

        logger.info(f"ğŸ¯ æ€»å…±æ¢å¤äº† {len(recovered_documents)} ä¸ªæ–‡æ¡£")
        return recovered_documents

    def _generate_final_report(self, source_summary: Dict) -> Dict:
        """ç”Ÿæˆæœ€ç»ˆæ„å»ºæŠ¥å‘Š"""
        duration = self.stats["end_time"] - self.stats["start_time"]

        report = {
            "build_info": {
                "build_time": time.strftime("%Y-%m-%d %H:%M:%S"),
                "duration_seconds": round(duration, 2),
                "duration_minutes": round(duration / 60, 2)
            },
            "statistics": self.stats,
            "sources": source_summary,
            "quality_metrics": {
                "success_rate": self.stats["total_files_valid"] / max(self.stats["total_files_found"], 1),
                "processing_rate": self.stats["total_files_processed"] / max(self.stats["total_files_found"], 1),
                "error_rate": self.stats["processing_errors"] / max(self.stats["total_files_found"], 1)
            },
            "dataset_info": {
                "total_documents": self.stats["total_files_valid"],
                "total_sources": len(source_summary),
                "average_docs_per_source": self.stats["total_files_valid"] / max(len(source_summary), 1)
            }
        }

        return report

    def _print_final_summary(self, report: Dict):
        """æ‰“å°æœ€ç»ˆæ‘˜è¦"""
        print("\n" + "=" * 60)
        print("ğŸ“Š SMART RAG æ•°æ®é›†æ„å»ºæŠ¥å‘Š")
        print("=" * 60)

        print(f"â±ï¸  æ„å»ºæ—¶é—´: {report['build_info']['duration_minutes']:.1f} åˆ†é’Ÿ")
        print(f"ğŸ“ æ•°æ®æºæ•°é‡: {report['dataset_info']['total_sources']}")
        print(f"ğŸ“„ æœ‰æ•ˆæ–‡æ¡£: {report['dataset_info']['total_documents']}")
        print(f"âœ… æˆåŠŸç‡: {report['quality_metrics']['success_rate']:.1%}")

        print("\nğŸ“‹ å„æ•°æ®æºç»Ÿè®¡:")
        for source, info in report['sources'].items():
            print(f"  {source}: {info['document_count']} æ–‡æ¡£")

        print("\nğŸ’¾ è¾“å‡ºæ–‡ä»¶:")
        print(f"  ğŸ“„ å®Œæ•´æ•°æ®é›†: processed_data/complete_dataset.json")
        print(f"  ğŸ“Š æ„å»ºæŠ¥å‘Š: processed_data/quality_reports/build_report.json")
        print("=" * 60)


if __name__ == "__main__":
    # ç›´æ¥è¿è¡Œæµ‹è¯•
    builder = SmartRAGDatasetBuilder()
    builder.build_complete_dataset()