#!/usr/bin/env python3
"""
Smart RAG æ•°æ®é›†è´¨é‡éªŒè¯å·¥å…·
éªŒè¯æ„å»ºçš„æ•°æ®é›†æ˜¯å¦ç¬¦åˆPhase 2çš„éœ€æ±‚
"""

import json
import pandas as pd
from pathlib import Path
from typing import Dict, List, Tuple
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DatasetQualityValidator:
    """æ•°æ®é›†è´¨é‡éªŒè¯å™¨"""

    def __init__(self, dataset_path: str = "processed_data/complete_dataset.json"):
        self.dataset_path = Path(dataset_path)
        self.dataset = self._load_dataset()
        self.validation_results = {}

    def _load_dataset(self) -> List[Dict]:
        """åŠ è½½æ•°æ®é›†"""
        if not self.dataset_path.exists():
            raise FileNotFoundError(f"æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {self.dataset_path}")

        with open(self.dataset_path, 'r', encoding='utf-8') as f:
            dataset = json.load(f)

        logger.info(f"ğŸ“Š å·²åŠ è½½æ•°æ®é›†: {len(dataset)} ä¸ªæ–‡æ¡£")
        return dataset

    def validate_basic_structure(self) -> Dict:
        """éªŒè¯åŸºæœ¬æ•°æ®ç»“æ„"""
        logger.info("ğŸ” éªŒè¯åŸºæœ¬æ•°æ®ç»“æ„...")

        required_fields = ["id", "source", "title", "content", "headings", "code_blocks", "stats"]
        validation_results = {
            "total_documents": len(self.dataset),
            "valid_documents": 0,
            "missing_fields": [],
            "field_coverage": {}
        }

        # å¤„ç†ç©ºæ•°æ®é›†çš„æƒ…å†µ
        if len(self.dataset) == 0:
            logger.warning("âš ï¸  æ•°æ®é›†ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œç»“æ„éªŒè¯")
            validation_results["error"] = "Empty dataset"
            validation_results["field_coverage"] = {field: {"count": 0, "coverage": 0.0} for field in required_fields}
            validation_results["missing_fields"] = required_fields
            self.validation_results["structure"] = validation_results
            return validation_results

        for field in required_fields:
            field_count = sum(1 for doc in self.dataset if field in doc and doc[field] is not None)
            coverage = field_count / len(self.dataset)  # ç°åœ¨å®‰å…¨äº†ï¼Œå› ä¸ºå·²ç»æ£€æŸ¥äº†len(self.dataset) > 0
            validation_results["field_coverage"][field] = {
                "count": field_count,
                "coverage": coverage
            }

            if coverage < 0.95:  # 95%è¦†ç›–ç‡æ ‡å‡†
                validation_results["missing_fields"].append(field)

        validation_results["valid_documents"] = sum(
            1 for doc in self.dataset
            if all(field in doc and doc[field] is not None for field in required_fields)
        )

        self.validation_results["structure"] = validation_results
        return validation_results

    def validate_content_quality(self) -> Dict:
        """éªŒè¯å†…å®¹è´¨é‡"""
        logger.info("ğŸ“ éªŒè¯å†…å®¹è´¨é‡...")

        quality_metrics = {
            "content_length": [],
            "heading_count": [],
            "code_block_count": [],
            "word_count": [],
            "code_ratio": []
        }

        for doc in self.dataset:
            if "stats" in doc and doc["stats"]:
                stats = doc["stats"]
                quality_metrics["content_length"].append(stats.get("character_count", 0))
                quality_metrics["word_count"].append(stats.get("word_count", 0))
                quality_metrics["code_ratio"].append(stats.get("code_ratio", 0))

            quality_metrics["heading_count"].append(len(doc.get("headings", [])))
            quality_metrics["code_block_count"].append(len(doc.get("code_blocks", [])))

        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        quality_results = {}
        for metric, values in quality_metrics.items():
            if values:
                quality_results[metric] = {
                    "mean": sum(values) / len(values),
                    "min": min(values),
                    "max": max(values),
                    "median": sorted(values)[len(values) // 2]
                }
            else:
                quality_results[metric] = {"error": "No valid data"}

        # è´¨é‡æ£€æŸ¥
        quality_issues = []

        # æ£€æŸ¥è¿‡çŸ­æ–‡æ¡£
        short_docs = sum(1 for length in quality_metrics["content_length"] if length < 200)
        if short_docs > len(self.dataset) * 0.1:  # è¶…è¿‡10%çš„æ–‡æ¡£è¿‡çŸ­
            quality_issues.append(f"è¿‡çŸ­æ–‡æ¡£æ¯”ä¾‹è¿‡é«˜: {short_docs}/{len(self.dataset)}")

        # æ£€æŸ¥æ— æ ‡é¢˜æ–‡æ¡£
        no_heading_docs = sum(1 for count in quality_metrics["heading_count"] if count == 0)
        if no_heading_docs > len(self.dataset) * 0.05:  # è¶…è¿‡5%çš„æ–‡æ¡£æ— æ ‡é¢˜
            quality_issues.append(f"æ— æ ‡é¢˜æ–‡æ¡£æ¯”ä¾‹è¿‡é«˜: {no_heading_docs}/{len(self.dataset)}")

        quality_results["issues"] = quality_issues
        quality_results["summary"] = {
            "total_characters": sum(quality_metrics["content_length"]),
            "total_words": sum(quality_metrics["word_count"]),
            "total_headings": sum(quality_metrics["heading_count"]),
            "total_code_blocks": sum(quality_metrics["code_block_count"])
        }

        self.validation_results["quality"] = quality_results
        return quality_results

    def validate_source_distribution(self) -> Dict:
        """éªŒè¯æ•°æ®æºåˆ†å¸ƒ"""
        logger.info("ğŸ“Š éªŒè¯æ•°æ®æºåˆ†å¸ƒ...")

        source_distribution = Counter(doc.get("source", "unknown") for doc in self.dataset)

        distribution_results = {
            "sources": dict(source_distribution),
            "total_sources": len(source_distribution),
            "balance_score": self._calculate_balance_score(source_distribution) if source_distribution else 0.0
        }

        # æ£€æŸ¥æ˜¯å¦æœ‰æºå æ¯”è¿‡é«˜
        total_docs = len(self.dataset)
        if total_docs > 0:  # é¿å…é™¤é›¶
            max_ratio = max(count / total_docs for count in source_distribution.values()) if source_distribution else 0

            if max_ratio > 0.7:  # å•ä¸€æ•°æ®æºå æ¯”è¶…è¿‡70%
                distribution_results["warning"] = f"æ•°æ®æºåˆ†å¸ƒä¸å‡è¡¡ï¼Œæœ€å¤§å æ¯”: {max_ratio:.1%}"
        else:
            distribution_results["warning"] = "æ•°æ®é›†ä¸ºç©ºï¼Œæ— æ³•è¯„ä¼°åˆ†å¸ƒ"

        self.validation_results["distribution"] = distribution_results
        return distribution_results

    def _calculate_balance_score(self, distribution: Counter) -> float:
        """è®¡ç®—åˆ†å¸ƒå‡è¡¡åº¦åˆ†æ•° (0-1, è¶Šæ¥è¿‘1è¶Šå‡è¡¡)"""
        if not distribution:
            return 0.0

        total = sum(distribution.values())
        expected_ratio = 1.0 / len(distribution)

        # è®¡ç®—æ¯ä¸ªæºä¸æœŸæœ›æ¯”ä¾‹çš„åå·®
        deviations = []
        for count in distribution.values():
            actual_ratio = count / total
            deviation = abs(actual_ratio - expected_ratio)
            deviations.append(deviation)

        # å‡è¡¡åº¦ = 1 - å¹³å‡åå·®
        avg_deviation = sum(deviations) / len(deviations)
        balance_score = max(0, 1 - avg_deviation * len(distribution))

        return balance_score

    def validate_phase2_readiness(self) -> Dict:
        """éªŒè¯Phase 2å®šåˆ¶splittingçš„å‡†å¤‡åº¦"""
        logger.info("ğŸ¯ éªŒè¯Phase 2å‡†å¤‡åº¦...")

        phase2_metrics = {
            "structural_features": 0,
            "code_diversity": 0,
            "heading_hierarchy": 0,
            "cross_references": 0
        }

        # å¤„ç†ç©ºæ•°æ®é›†
        if not self.dataset:
            logger.warning("âš ï¸  æ•°æ®é›†ä¸ºç©ºï¼Œæ— æ³•è¯„ä¼°Phase 2å‡†å¤‡åº¦")
            phase2_metrics.update({
                "structural_coverage": 0.0,
                "code_language_diversity": 0,
                "heading_level_diversity": 0,
                "cross_reference_coverage": 0.0,
                "languages_found": [],
                "heading_levels_found": [],
                "readiness_score": 0,
                "readiness_factors": ["âŒ æ•°æ®é›†ä¸ºç©º"],
                "readiness_level": "éœ€è¦é‡å»ºæ•°æ®é›†"
            })
            self.validation_results["phase2_readiness"] = phase2_metrics
            return phase2_metrics

        # æ£€æŸ¥ç»“æ„åŒ–ç‰¹å¾
        docs_with_structure = 0
        code_languages = set()
        heading_levels = set()
        docs_with_links = 0

        for doc in self.dataset:
            # ç»“æ„åŒ–æ–‡æ¡£ (æœ‰æ ‡é¢˜å’Œå†…å®¹)
            if doc.get("headings") and len(doc["headings"]) > 0:
                docs_with_structure += 1

                # æ”¶é›†æ ‡é¢˜å±‚çº§
                for heading in doc["headings"]:
                    heading_levels.add(heading.get("level", 1))

            # ä»£ç å—å¤šæ ·æ€§
            if doc.get("code_blocks"):
                for block in doc["code_blocks"]:
                    code_languages.add(block.get("language", "text"))

            # äº¤å‰å¼•ç”¨ (é“¾æ¥)
            if doc.get("links") and len(doc["links"]) > 0:
                docs_with_links += 1

        total_docs = len(self.dataset)

        phase2_metrics = {
            "structural_coverage": docs_with_structure / total_docs,
            "code_language_diversity": len(code_languages),
            "heading_level_diversity": len(heading_levels),
            "cross_reference_coverage": docs_with_links / total_docs,
            "languages_found": list(code_languages),
            "heading_levels_found": sorted(list(heading_levels))
        }

        # è¯„ä¼°å‡†å¤‡åº¦
        readiness_score = 0
        readiness_factors = []

        if phase2_metrics["structural_coverage"] > 0.8:
            readiness_score += 25
            readiness_factors.append("âœ… è‰¯å¥½çš„æ–‡æ¡£ç»“æ„è¦†ç›–")
        else:
            readiness_factors.append("âš ï¸ ç»“æ„åŒ–æ–‡æ¡£æ¯”ä¾‹åä½")

        if phase2_metrics["code_language_diversity"] >= 5:
            readiness_score += 25
            readiness_factors.append("âœ… ä»£ç è¯­è¨€å¤šæ ·æ€§å……è¶³")
        else:
            readiness_factors.append("âš ï¸ ä»£ç è¯­è¨€ç±»å‹è¾ƒå°‘")

        if phase2_metrics["heading_level_diversity"] >= 3:
            readiness_score += 25
            readiness_factors.append("âœ… æ ‡é¢˜å±‚æ¬¡ä¸°å¯Œ")
        else:
            readiness_factors.append("âš ï¸ æ ‡é¢˜å±‚æ¬¡å•ä¸€")

        if phase2_metrics["cross_reference_coverage"] > 0.3:
            readiness_score += 25
            readiness_factors.append("âœ… äº¤å‰å¼•ç”¨è¦†ç›–è‰¯å¥½")
        else:
            readiness_factors.append("âš ï¸ äº¤å‰å¼•ç”¨è¾ƒå°‘")

        phase2_metrics["readiness_score"] = readiness_score
        phase2_metrics["readiness_factors"] = readiness_factors
        phase2_metrics["readiness_level"] = self._get_readiness_level(readiness_score)

        self.validation_results["phase2_readiness"] = phase2_metrics
        return phase2_metrics

    def _get_readiness_level(self, score: int) -> str:
        """è·å–å‡†å¤‡åº¦ç­‰çº§"""
        if score >= 90:
            return "ä¼˜ç§€ (Excellent)"
        elif score >= 75:
            return "è‰¯å¥½ (Good)"
        elif score >= 60:
            return "ä¸­ç­‰ (Fair)"
        else:
            return "éœ€è¦æ”¹è¿› (Needs Improvement)"

    def generate_validation_report(self) -> Dict:
        """ç”Ÿæˆå®Œæ•´éªŒè¯æŠ¥å‘Š"""
        logger.info("ğŸ“‹ ç”ŸæˆéªŒè¯æŠ¥å‘Š...")

        # è¿è¡Œæ‰€æœ‰éªŒè¯
        structure_results = self.validate_basic_structure()
        quality_results = self.validate_content_quality()
        distribution_results = self.validate_source_distribution()
        phase2_results = self.validate_phase2_readiness()

        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        report = {
            "validation_timestamp": pd.Timestamp.now().isoformat(),
            "dataset_info": {
                "file_path": str(self.dataset_path),
                "total_documents": len(self.dataset)
            },
            "validation_results": {
                "structure": structure_results,
                "quality": quality_results,
                "distribution": distribution_results,
                "phase2_readiness": phase2_results
            },
            "overall_assessment": self._generate_overall_assessment()
        }

        # ä¿å­˜æŠ¥å‘Š
        report_path = Path("processed_data/quality_reports/validation_report.json")
        report_path.parent.mkdir(parents=True, exist_ok=True)

        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        logger.info(f"ğŸ“Š éªŒè¯æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        return report

    def _generate_overall_assessment(self) -> Dict:
        """ç”Ÿæˆæ€»ä½“è¯„ä¼°"""
        assessments = []
        score = 0

        # å¤„ç†ç©ºæ•°æ®é›†æƒ…å†µ
        if not self.dataset:
            return {
                "overall_score": 0,
                "grade": "F (æ•°æ®é›†ä¸ºç©º)",
                "assessments": ["âŒ æ•°æ®é›†ä¸ºç©ºï¼Œéœ€è¦é‡æ–°æ„å»º"],
                "recommendations": ["é‡æ–°è¿è¡Œæ•°æ®é›†æ„å»ºæµç¨‹", "æ£€æŸ¥æ•°æ®æºé…ç½®", "éªŒè¯ç½‘ç»œè¿æ¥"]
            }

        # ç»“æ„å®Œæ•´æ€§
        if self.validation_results["structure"]["valid_documents"] / len(self.dataset) > 0.95:
            assessments.append("âœ… æ•°æ®ç»“æ„å®Œæ•´")
            score += 25
        else:
            assessments.append("âš ï¸ æ•°æ®ç»“æ„å­˜åœ¨ç¼ºå¤±")

        # å†…å®¹è´¨é‡
        if not self.validation_results["quality"].get("issues"):
            assessments.append("âœ… å†…å®¹è´¨é‡è‰¯å¥½")
            score += 25
        else:
            assessments.append("âš ï¸ å†…å®¹è´¨é‡å­˜åœ¨é—®é¢˜")

        # åˆ†å¸ƒå‡è¡¡
        if self.validation_results["distribution"]["balance_score"] > 0.7:
            assessments.append("âœ… æ•°æ®æºåˆ†å¸ƒå‡è¡¡")
            score += 25
        else:
            assessments.append("âš ï¸ æ•°æ®æºåˆ†å¸ƒä¸å‡è¡¡")

        # Phase 2å‡†å¤‡åº¦
        phase2_score = self.validation_results["phase2_readiness"]["readiness_score"]
        if phase2_score >= 75:
            assessments.append("âœ… Phase 2å‡†å¤‡åº¦å……åˆ†")
            score += 25
        else:
            assessments.append("âš ï¸ Phase 2å‡†å¤‡åº¦éœ€è¦æå‡")

        return {
            "overall_score": score,
            "grade": self._get_grade(score),
            "assessments": assessments,
            "recommendations": self._generate_recommendations()
        }

    def _get_grade(self, score: int) -> str:
        """è·å–æ€»ä½“è¯„çº§"""
        if score >= 90:
            return "A (ä¼˜ç§€)"
        elif score >= 80:
            return "B (è‰¯å¥½)"
        elif score >= 70:
            return "C (ä¸­ç­‰)"
        else:
            return "D (éœ€è¦æ”¹è¿›)"

    def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []

        # åŸºäºéªŒè¯ç»“æœç”Ÿæˆå»ºè®®
        if self.validation_results["structure"]["missing_fields"]:
            recommendations.append("è¡¥å……ç¼ºå¤±çš„æ•°æ®å­—æ®µ")

        if self.validation_results["quality"].get("issues"):
            recommendations.append("æå‡å†…å®¹è´¨é‡ï¼Œç§»é™¤è¿‡çŸ­æˆ–æ— æ•ˆæ–‡æ¡£")

        if self.validation_results["distribution"]["balance_score"] < 0.7:
            recommendations.append("å¹³è¡¡æ•°æ®æºåˆ†å¸ƒï¼Œå¢åŠ è¾ƒå°‘çš„æ•°æ®æºå†…å®¹")

        if self.validation_results["phase2_readiness"]["readiness_score"] < 75:
            recommendations.append("å¢å¼ºæ–‡æ¡£ç»“æ„å¤šæ ·æ€§ï¼Œå‡†å¤‡Phase 2å®šåˆ¶splitting")

        if not recommendations:
            recommendations.append("æ•°æ®é›†è´¨é‡è‰¯å¥½ï¼Œå¯ä»¥å¼€å§‹Phase 1å®æ–½")

        return recommendations

    def print_summary(self):
        """æ‰“å°éªŒè¯æ‘˜è¦"""
        report = self.generate_validation_report()

        print("\n" + "=" * 60)
        print("ğŸ“Š SMART RAG æ•°æ®é›†è´¨é‡éªŒè¯æŠ¥å‘Š")
        print("=" * 60)

        # åŸºæœ¬ä¿¡æ¯
        total_docs = report['dataset_info']['total_documents']
        print(f"ğŸ“„ æ€»æ–‡æ¡£æ•°: {total_docs}")

        if total_docs == 0:
            print("âŒ æ•°æ®é›†ä¸ºç©ºï¼")
            print("\nğŸ’¡ å»ºè®®:")
            print("  1. æ£€æŸ¥æ˜¯å¦å­˜åœ¨å•ç‹¬çš„æ•°æ®æºæ–‡ä»¶:")
            print("     dir processed_data\\documents")
            print("  2. å¦‚æœå­˜åœ¨ï¼Œè¿è¡Œæ•°æ®æ¢å¤:")
            print("     python recover_dataset.py")
            print("  3. å¦‚æœä¸å­˜åœ¨ï¼Œé‡æ–°æ„å»ºæ•°æ®é›†:")
            print("     python build_dataset.py")
        else:
            print(f"ğŸ“Š æ€»ä½“è¯„çº§: {report['overall_assessment']['grade']}")

            # åªæœ‰åœ¨æ•°æ®ä¸ä¸ºç©ºæ—¶æ‰æ˜¾ç¤ºPhase 2å‡†å¤‡åº¦
            if 'phase2_readiness' in self.validation_results and 'readiness_level' in self.validation_results[
                'phase2_readiness']:
                print(f"ğŸ¯ Phase 2å‡†å¤‡åº¦: {self.validation_results['phase2_readiness']['readiness_level']}")

            # æ•°æ®æºåˆ†å¸ƒ
            if report['validation_results']['distribution']['sources']:
                print(f"\nğŸ“‹ æ•°æ®æºåˆ†å¸ƒ:")
                for source, count in report['validation_results']['distribution']['sources'].items():
                    percentage = count / total_docs * 100
                    print(f"  {source}: {count} ({percentage:.1f}%)")

            # è¯„ä¼°ç»“æœ
            print(f"\nâœ… è¯„ä¼°ç»“æœ:")
            for assessment in report['overall_assessment']['assessments']:
                print(f"  {assessment}")

        # æ”¹è¿›å»ºè®®
        print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
        for recommendation in report['overall_assessment']['recommendations']:
            print(f"  â€¢ {recommendation}")

        print("=" * 60)


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="Smart RAG æ•°æ®é›†è´¨é‡éªŒè¯")
    parser.add_argument(
        "--dataset",
        type=str,
        default="processed_data/complete_dataset.json",
        help="æ•°æ®é›†æ–‡ä»¶è·¯å¾„"
    )

    args = parser.parse_args()

    try:
        validator = DatasetQualityValidator(args.dataset)
        validator.print_summary()

    except Exception as e:
        logger.error(f"âŒ éªŒè¯è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {e}")
        return 1

    return 0


if __name__ == "__main__":
    import sys

    sys.exit(main())