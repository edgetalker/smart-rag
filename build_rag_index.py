import os
from dotenv import load_dotenv
import json
import time
from datetime import datetime
import hashlib
from typing import List, Dict, Any
from pathlib import Path
from dataclasses import dataclass
from tqdm import tqdm

# å¯¼å…¥ä¾èµ–åŒ…
import google.generativeai as genai
from langchain.text_splitter import RecursiveCharacterTextSplitter
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct
from elasticsearch import Elasticsearch

# ============================================================================
# é…ç½®éƒ¨åˆ†
# ============================================================================
load_dotenv()

# åŸºç¡€é…ç½®
DATASET_PATH = os.getenv('DATASET_PATH')
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')

# LangChainåˆ†å‰²å™¨é…ç½®
CHUNK_SIZE = os.getenv('CHUNK_SIZE')
CHUNK_OVERLAP = os.getenv('CHUNK_OVERLAP')

# Geminié…ç½®
GEMINI_MODEL = os.getenv('GEMINI_MODEL')
EMBEDDING_DIMENSION = int(os.getenv('EMBEDDING_DIMENSION'))
BATCH_SIZE = int(os.getenv('BATCH_SIZE'))

# Qdranté…ç½®
QDRANT_HOST = os.getenv("QDRANT_HOST", "localhost")
QDRANT_PORT = int(os.getenv("QDRANT_PORT", 6333))
QDRANT_COLLECTION = os.getenv("QDRANT_COLLECTION")

# Elasticsearché…ç½®
ES_HOSTS = ["http://localhost:9200"]
ES_INDEX = os.getenv("ES_INDEX")

# å¤„ç†é…ç½®
MAX_DOCUMENTS = None
SKIP_EMPTY_DOCS = True
MIN_CONTENT_LENGTH = 50

# å†…å®¹é™åˆ¶é…ç½®
MAX_CONTENT_LENGTH = 10000
MAX_TITLE_LENGTH = 200


@dataclass
class ProcessedChunk:
    """å¤„ç†åçš„æ–‡æ¡£å—"""
    chunk_id: str
    content: str
    embedding: List[float]
    metadata: Dict[str, Any]


# ============================================================================
# å·¥å…·å‡½æ•°
# ============================================================================

def setup_gemini_api():
    """è®¾ç½®Gemini API"""
    if not GEMINI_API_KEY:
        print("âŒ é”™è¯¯: è¯·è®¾ç½®GEMINI_API_KEYç¯å¢ƒå˜é‡")
        print("   è·å–API Key: https://ai.google.dev/")
        exit(1)

    genai.configure(api_key=GEMINI_API_KEY)

    # æµ‹è¯•è¿æ¥
    try:
        result = genai.embed_content(
            model=GEMINI_MODEL,
            content="æµ‹è¯•è¿æ¥",
            task_type="retrieval_document"
        )
        if result and 'embedding' in result:
            print("âœ… Gemini APIè¿æ¥æˆåŠŸ")
            return True
    except Exception as e:
        print(f"âŒ Gemini APIè¿æ¥å¤±è´¥: {e}")
        return False


def load_documents():
    """åŠ è½½æ–‡æ¡£æ•°æ®é›†"""
    dataset_file = Path(DATASET_PATH)

    if not dataset_file.exists():
        print(f"âŒ æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {DATASET_PATH}")
        print("   è¯·å…ˆè¿è¡Œæ•°æ®é›†æ„å»ºè„šæœ¬")
        exit(1)

    print(f"ğŸ“š åŠ è½½æ•°æ®é›†: {DATASET_PATH}")

    with open(dataset_file, 'r', encoding='utf-8') as f:
        documents = json.load(f)

    # æ•°æ®è¿‡æ»¤
    filtered_docs = []
    for doc in documents:
        # è·³è¿‡ç©ºå†…å®¹
        if SKIP_EMPTY_DOCS and not doc.get('content', '').strip():
            continue

        # æ£€æŸ¥æœ€å°é•¿åº¦
        if len(doc.get('content', '')) < MIN_CONTENT_LENGTH:
            continue

        filtered_docs.append(doc)

    # é™åˆ¶æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
    if MAX_DOCUMENTS:
        filtered_docs = filtered_docs[:MAX_DOCUMENTS]

    print(f"ğŸ“Š åŠ è½½å®Œæˆ: {len(filtered_docs)} ä¸ªæ–‡æ¡£ï¼ˆåŸå§‹: {len(documents)}ï¼‰")
    return filtered_docs


def split_documents(documents):
    """ä½¿ç”¨LangChainåˆ†å‰²æ–‡æ¡£"""
    print("ğŸ“„ å¼€å§‹æ–‡æ¡£åˆ†å‰²...")

    # åˆå§‹åŒ–LangChainåˆ†å‰²å™¨
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        length_function=len,
        separators=["\n\n", "\n", " ", ""],
        keep_separator=True
    )

    all_chunks = []

    for doc in tqdm(documents, desc="åˆ†å‰²æ–‡æ¡£"):
        doc_id = doc.get('id', 'unknown')
        content = doc.get('content', '')

        if not content.strip():
            continue

        try:
            # ä½¿ç”¨LangChainåˆ†å‰²
            text_chunks = text_splitter.split_text(content)

            # ä¸ºæ¯ä¸ªå—åˆ›å»ºå…ƒæ•°æ®
            for i, chunk_content in enumerate(text_chunks):
                chunk_id = f"{doc_id}_chunk_{i}_{hashlib.md5(chunk_content.encode()).hexdigest()[:8]}"

                chunk_metadata = {
                    'chunk_id': chunk_id,
                    'source_id': doc_id,
                    'source': doc.get('source', 'unknown'),
                    'title': doc.get('title', ''),
                    'chunk_index': i,
                    'chunk_length': len(chunk_content),
                    'original_doc_length': len(content),
                    'has_code_blocks': len(doc.get('code_blocks', [])) > 0,
                    'created_at': datetime.now().isoformat()
                }

                chunk_data = {
                    'chunk_id': chunk_id,
                    'content': chunk_content,
                    'metadata': chunk_metadata
                }

                all_chunks.append(chunk_data)

        except Exception as e:
            print(f"âš ï¸ æ–‡æ¡£ {doc_id} åˆ†å‰²å¤±è´¥: {e}")
            continue

    print(f"âœ… åˆ†å‰²å®Œæˆ: {len(documents)} æ–‡æ¡£ -> {len(all_chunks)} å—")
    return all_chunks


def generate_embeddings(chunks):
    """ä½¿ç”¨Geminiç”ŸæˆåµŒå…¥å‘é‡"""
    print("ğŸ§  å¼€å§‹ç”ŸæˆåµŒå…¥å‘é‡...")

    embeddings = []
    failed_count = 0

    # æ‰¹é‡å¤„ç†
    for i in tqdm(range(0, len(chunks), BATCH_SIZE), desc="ç”ŸæˆåµŒå…¥"):
        batch_chunks = chunks[i:i + BATCH_SIZE]

        for chunk in batch_chunks:
            try:
                # è°ƒç”¨Gemini API
                result = genai.embed_content(
                    model=GEMINI_MODEL,
                    content=chunk['content'],
                    task_type="retrieval_document"
                )

                if result and 'embedding' in result:
                    embeddings.append(result['embedding'])
                else:
                    # å¤±è´¥æ—¶æ·»åŠ é›¶å‘é‡
                    embeddings.append([0.0] * EMBEDDING_DIMENSION)
                    failed_count += 1

            except Exception as e:
                print(f"âš ï¸ åµŒå…¥ç”Ÿæˆå¤±è´¥: {str(e)[:50]}...")
                embeddings.append([0.0] * EMBEDDING_DIMENSION)
                failed_count += 1

            # é¿å…APIé¢‘ç‡é™åˆ¶
            time.sleep(0.1)

        # æ‰¹æ¬¡é—´ä¼‘æ¯
        if i > 0:
            time.sleep(1)

    success_count = len(embeddings) - failed_count
    print(f"âœ… åµŒå…¥ç”Ÿæˆå®Œæˆ: {success_count}/{len(chunks)} æˆåŠŸ")

    if failed_count > 0:
        print(f"âš ï¸ å¤±è´¥æ•°é‡: {failed_count}")

    return embeddings


def create_processed_chunks(chunks, embeddings):
    """ç»„åˆæ–‡æ¡£å—å’ŒåµŒå…¥å‘é‡"""
    print("ğŸ”§ ç»„åˆæ–‡æ¡£å—å’ŒåµŒå…¥å‘é‡...")

    processed_chunks = []

    for chunk, embedding in zip(chunks, embeddings):
        processed_chunk = ProcessedChunk(
            chunk_id=chunk['chunk_id'],
            content=chunk['content'],
            embedding=embedding,
            metadata=chunk['metadata']
        )
        processed_chunks.append(processed_chunk)

    print(f"âœ… ç»„åˆå®Œæˆ: {len(processed_chunks)} ä¸ªå¤„ç†å—")
    return processed_chunks


def setup_qdrant_v17():
    """Qdrant 1.7ç‰ˆæœ¬ä¸“ç”¨è®¾ç½®"""
    print("ğŸ—„ï¸ è®¾ç½®Qdrantå‘é‡æ•°æ®åº“...")

    try:
        # 1.7ç‰ˆæœ¬çš„ç®€åŒ–è¿æ¥ï¼ˆç§»é™¤ä¸å…¼å®¹å‚æ•°ï¼‰
        client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)

        # æ£€æŸ¥è¿æ¥
        try:
            collections = client.get_collections()
            print(f"âœ… Qdrantè¿æ¥æˆåŠŸï¼Œå½“å‰é›†åˆæ•°: {len(collections.collections)}")
        except Exception as e:
            print(f"âš ï¸ è¿æ¥æµ‹è¯•è­¦å‘Š: {e}")

        # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
        collection_names = [col.name for col in collections.collections]

        if QDRANT_COLLECTION in collection_names:
            print(f"âš ï¸ é›†åˆ {QDRANT_COLLECTION} å·²å­˜åœ¨ï¼Œå°†åˆ é™¤é‡å»º")
            client.delete_collection(QDRANT_COLLECTION)

        # åˆ›å»ºæ–°é›†åˆ
        client.create_collection(
            collection_name=QDRANT_COLLECTION,
            vectors_config=VectorParams(
                size=EMBEDDING_DIMENSION,
                distance=Distance.COSINE
            )
        )

        print(f"âœ… Qdranté›†åˆåˆ›å»ºæˆåŠŸ: {QDRANT_COLLECTION}")
        return client

    except Exception as e:
        print(f"âŒ Qdrantè®¾ç½®å¤±è´¥: {e}")
        print("ğŸ’¡ è¯·ç¡®ä¿QdrantæœåŠ¡æ­£åœ¨è¿è¡Œ: docker run -p 6333:6333 qdrant/qdrant")
        return None


def generate_safe_id(chunk_id: str, use_numeric: bool = False):
    """ç”Ÿæˆå®‰å…¨çš„ID"""
    if use_numeric:
        # æ•°å­—IDæ–¹æ¡ˆ
        return hash(chunk_id) % (2 ** 31)
    else:
        # å­—ç¬¦ä¸²IDæ–¹æ¡ˆ - æ¸…ç†ç‰¹æ®Šå­—ç¬¦
        safe_id = chunk_id.replace('/', '_').replace('\\', '_').replace(' ', '_')
        # é™åˆ¶é•¿åº¦
        if len(safe_id) > 100:
            safe_id = safe_id[:90] + "_" + hashlib.md5(safe_id.encode()).hexdigest()[:8]
        return safe_id


def build_qdrant_index_v17(client, processed_chunks):
    """Qdrant 1.7ç‰ˆæœ¬ä¸“ç”¨ç´¢å¼•æ„å»º"""
    print("ğŸ”„ æ„å»ºQdrantå‘é‡ç´¢å¼•ï¼ˆ1.7ç‰ˆæœ¬å…¼å®¹ï¼‰...")

    if not client:
        print("âŒ Qdrantå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
        return False

    # é¦–å…ˆå°è¯•å­—ç¬¦ä¸²IDæ–¹æ¡ˆ
    success = try_string_id_upload(client, processed_chunks)

    if not success:
        print("ğŸ”§ å­—ç¬¦ä¸²IDå¤±è´¥ï¼Œå°è¯•æ•°å­—IDæ–¹æ¡ˆ...")
        success = try_numeric_id_upload(client, processed_chunks)

    return success


def try_string_id_upload(client, processed_chunks):
    """å°è¯•å­—ç¬¦ä¸²IDä¸Šä¼ """
    print("ğŸ”§ å°è¯•å­—ç¬¦ä¸²IDæ–¹æ¡ˆ...")

    try:
        points = []

        # å‡†å¤‡å‰10ä¸ªç‚¹è¿›è¡Œæµ‹è¯•
        test_chunks = processed_chunks[:10]

        for chunk in test_chunks:
            safe_id = generate_safe_id(chunk.chunk_id, use_numeric=False)
            content = chunk.content[:MAX_CONTENT_LENGTH]
            title = chunk.metadata.get("title", "")[:MAX_TITLE_LENGTH]

            point = PointStruct(
                id=safe_id,
                vector=chunk.embedding,
                payload={
                    "content": content,
                    "source_id": chunk.metadata["source_id"],
                    "source": chunk.metadata["source"],
                    "title": title,
                    "chunk_index": chunk.metadata["chunk_index"],
                    "chunk_length": chunk.metadata["chunk_length"],
                    "has_code_blocks": chunk.metadata["has_code_blocks"],
                    "created_at": chunk.metadata["created_at"]
                }
            )
            points.append(point)

        # æµ‹è¯•ä¸Šä¼ 
        client.upsert(
            collection_name=QDRANT_COLLECTION,
            points=points
        )

        print("âœ… å­—ç¬¦ä¸²IDæµ‹è¯•æˆåŠŸï¼Œç»§ç»­å…¨é‡ä¸Šä¼ ...")

        # å…¨é‡ä¸Šä¼ 
        return upload_all_chunks_string_id(client, processed_chunks)

    except Exception as e:
        print(f"âš ï¸ å­—ç¬¦ä¸²IDæ–¹æ¡ˆå¤±è´¥: {e}")
        return False


def upload_all_chunks_string_id(client, processed_chunks):
    """å­—ç¬¦ä¸²IDå…¨é‡ä¸Šä¼ """
    try:
        points = []
        failed_count = 0

        for chunk in tqdm(processed_chunks, desc="å‡†å¤‡å­—ç¬¦ä¸²IDæ•°æ®"):
            try:
                safe_id = generate_safe_id(chunk.chunk_id, use_numeric=False)
                content = chunk.content[:MAX_CONTENT_LENGTH]
                title = chunk.metadata.get("title", "")[:MAX_TITLE_LENGTH]

                point = PointStruct(
                    id=safe_id,
                    vector=chunk.embedding,
                    payload={
                        "original_chunk_id": chunk.chunk_id,
                        "content": content,
                        "source_id": chunk.metadata["source_id"],
                        "source": chunk.metadata["source"],
                        "title": title,
                        "chunk_index": chunk.metadata["chunk_index"],
                        "chunk_length": chunk.metadata["chunk_length"],
                        "has_code_blocks": chunk.metadata["has_code_blocks"],
                        "created_at": chunk.metadata["created_at"]
                    }
                )
                points.append(point)

            except Exception as e:
                failed_count += 1
                continue

        # åˆ†æ‰¹ä¸Šä¼ 
        batch_size = 20
        success_count = 0

        for i in tqdm(range(0, len(points), batch_size), desc="ä¸Šä¼ å­—ç¬¦ä¸²ID"):
            batch_points = points[i:i + batch_size]

            try:
                client.upsert(
                    collection_name=QDRANT_COLLECTION,
                    points=batch_points
                )
                success_count += len(batch_points)

            except Exception as e:
                print(f"âš ï¸ æ‰¹æ¬¡ {i // batch_size + 1} å¤±è´¥: {e}")
                continue

        success_rate = (success_count / len(points) * 100) if len(points) > 0 else 0
        print(f"âœ… å­—ç¬¦ä¸²IDä¸Šä¼ å®Œæˆ: {success_count}/{len(points)} ({success_rate:.1f}%)")

        return success_rate > 90

    except Exception as e:
        print(f"âŒ å­—ç¬¦ä¸²IDå…¨é‡ä¸Šä¼ å¤±è´¥: {e}")
        return False


def try_numeric_id_upload(client, processed_chunks):
    """å°è¯•æ•°å­—IDä¸Šä¼ """
    print("ğŸ”§ ä½¿ç”¨æ•°å­—IDæ–¹æ¡ˆ...")

    try:
        points = []
        id_mapping = {}

        for chunk in tqdm(processed_chunks, desc="å‡†å¤‡æ•°å­—IDæ•°æ®"):
            try:
                numeric_id = generate_safe_id(chunk.chunk_id, use_numeric=True)
                id_mapping[str(numeric_id)] = chunk.chunk_id

                content = chunk.content[:MAX_CONTENT_LENGTH]
                title = chunk.metadata.get("title", "")[:MAX_TITLE_LENGTH]

                point = PointStruct(
                    id=numeric_id,
                    vector=chunk.embedding,
                    payload={
                        "original_chunk_id": chunk.chunk_id,
                        "content": content,
                        "source_id": chunk.metadata["source_id"],
                        "source": chunk.metadata["source"],
                        "title": title,
                        "chunk_index": chunk.metadata["chunk_index"],
                        "chunk_length": chunk.metadata["chunk_length"],
                        "has_code_blocks": chunk.metadata["has_code_blocks"],
                        "created_at": chunk.metadata["created_at"]
                    }
                )
                points.append(point)

            except Exception as e:
                continue

        # ä¿å­˜IDæ˜ å°„
        mapping_file = Path("data/qdrant_id_mapping.json")
        mapping_file.parent.mkdir(exist_ok=True)
        with open(mapping_file, 'w', encoding='utf-8') as f:
            json.dump(id_mapping, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ IDæ˜ å°„å·²ä¿å­˜: {mapping_file}")

        # åˆ†æ‰¹ä¸Šä¼ 
        batch_size = 20
        success_count = 0

        for i in tqdm(range(0, len(points), batch_size), desc="ä¸Šä¼ æ•°å­—ID"):
            batch_points = points[i:i + batch_size]

            try:
                client.upsert(
                    collection_name=QDRANT_COLLECTION,
                    points=batch_points
                )
                success_count += len(batch_points)

            except Exception as e:
                print(f"âš ï¸ æ•°å­—IDæ‰¹æ¬¡ {i // batch_size + 1} å¤±è´¥: {e}")
                continue

        success_rate = (success_count / len(points) * 100) if len(points) > 0 else 0
        print(f"âœ… æ•°å­—IDä¸Šä¼ å®Œæˆ: {success_count}/{len(points)} ({success_rate:.1f}%)")

        return success_rate > 90

    except Exception as e:
        print(f"âŒ æ•°å­—IDæ–¹æ¡ˆä¹Ÿå¤±è´¥: {e}")
        return False


def setup_elasticsearch_fixed():
    """ä¿®å¤ç‰ˆæœ¬çš„Elasticsearchè®¾ç½®"""
    print("ğŸ” è®¾ç½®Elasticsearchå…¨æ–‡ç´¢å¼•")

    try:
        # è¿æ¥Elasticsearch
        es = Elasticsearch(ES_HOSTS)

        # æ£€æŸ¥è¿æ¥
        if not es.ping():
            print("âŒ æ— æ³•è¿æ¥åˆ°Elasticsearch")
            return None

        # åˆ é™¤ç°æœ‰ç´¢å¼•
        if es.indices.exists(index=ES_INDEX):
            print(f"âš ï¸ ç´¢å¼• {ES_INDEX} å·²å­˜åœ¨ï¼Œå°†åˆ é™¤é‡å»º")
            es.indices.delete(index=ES_INDEX)

        # ä¿®å¤çš„ç´¢å¼•æ˜ å°„
        mapping = {
            "mappings": {
                "properties": {
                    "content": {
                        "type": "text",
                        "analyzer": "standard",
                        "index_options": "docs"
                    },
                    "title": {
                        "type": "text",
                        "analyzer": "standard"
                    },
                    "source": {"type": "keyword"},
                    "source_id": {"type": "keyword"},
                    "chunk_index": {"type": "integer"},
                    "chunk_length": {"type": "integer"},
                    "has_code_blocks": {"type": "boolean"},
                    "created_at": {
                        "type": "date",
                        "format": "strict_date_optional_time_nanos"
                    }
                }
            },
            "settings": {
                "number_of_shards": 1,
                "number_of_replicas": 0,
                "max_result_window": 10000
            }
        }

        # åˆ›å»ºç´¢å¼•
        es.indices.create(index=ES_INDEX, body=mapping)
        print(f"âœ… Elasticsearchç´¢å¼•åˆ›å»ºæˆåŠŸ: {ES_INDEX}")
        return es

    except Exception as e:
        print(f"âŒ Elasticsearchè®¾ç½®å¤±è´¥: {e}")
        return None


def build_elasticsearch_index_fixed(es, processed_chunks):
    """ä¿®å¤ç‰ˆæœ¬çš„Elasticsearchç´¢å¼•æ„å»º"""
    print("ğŸ”„ æ„å»ºElasticsearchå…¨æ–‡ç´¢å¼•ï¼ˆå¢å¼ºç‰ˆï¼‰...")

    if not es:
        print("âŒ Elasticsearchå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
        return False

    try:
        from elasticsearch.helpers import bulk

        actions = []
        failed_docs = []

        for chunk in processed_chunks:
            try:
                # æ¸…ç†å’ŒéªŒè¯æ•°æ®
                safe_id = chunk.chunk_id.replace('/', '_').replace('\\', '_')
                content = chunk.content[:32000] if len(chunk.content) > 32000 else chunk.content
                title = chunk.metadata.get("title", "")[:500]

                # ç¡®ä¿created_atæ ¼å¼æ­£ç¡®
                created_at = chunk.metadata["created_at"]
                if isinstance(created_at, (int, float)):
                    created_at = datetime.fromtimestamp(created_at).isoformat()

                doc = {
                    "_index": ES_INDEX,
                    "_id": safe_id,
                    "_source": {
                        "content": content,
                        "source_id": chunk.metadata["source_id"],
                        "source": chunk.metadata["source"],
                        "title": title,
                        "chunk_index": chunk.metadata["chunk_index"],
                        "chunk_length": min(chunk.metadata["chunk_length"], 32000),
                        "has_code_blocks": chunk.metadata["has_code_blocks"],
                        "created_at": created_at
                    }
                }
                actions.append(doc)

            except Exception as e:
                failed_docs.append(chunk.chunk_id)
                continue

        print(f"ğŸ“Š å‡†å¤‡ç´¢å¼•: {len(actions)} ä¸ªæ–‡æ¡£, {len(failed_docs)} ä¸ªé¢„å¤„ç†å¤±è´¥")

        # æ‰¹é‡ç´¢å¼•
        try:
            success, failed = bulk(es, actions, stats_only=True)
            print(f"âœ… ESç´¢å¼•: {success} æˆåŠŸ, {len(failed_docs)} é¢„å¤„ç†å¤±è´¥")

            # åˆ·æ–°ç´¢å¼•
            es.indices.refresh(index=ES_INDEX)
            return True

        except Exception as e:
            print(f"âŒ æ‰¹é‡ç´¢å¼•å¤±è´¥: {e}")
            return False

    except Exception as e:
        print(f"âŒ Elasticsearchç´¢å¼•æ„å»ºå¤±è´¥: {e}")
        return False


def save_processing_results(processed_chunks):
    """ä¿å­˜å¤„ç†ç»“æœ"""
    print("ğŸ’¾ ä¿å­˜å¤„ç†ç»“æœ...")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path("data")
    output_dir.mkdir(exist_ok=True)

    # ä¿å­˜å¤„ç†åçš„æ•°æ®
    output_data = []
    for chunk in processed_chunks:
        chunk_data = {
            "chunk_id": chunk.chunk_id,
            "content": chunk.content,
            "embedding": chunk.embedding,
            "metadata": chunk.metadata
        }
        output_data.append(chunk_data)

    output_file = output_dir / "processed_chunks.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)

    file_size = output_file.stat().st_size / (1024 * 1024)  # MB
    print(f"âœ… å¤„ç†ç»“æœå·²ä¿å­˜: {output_file} ({file_size:.2f}MB)")


def print_summary(processed_chunks, qdrant_success, es_success, total_time):
    """æ‰“å°å¤„ç†æ‘˜è¦"""
    print("\n" + "=" * 60)
    print("ğŸ“Š Smart RAG ç´¢å¼•æ„å»ºæ‘˜è¦")
    print("=" * 60)

    # åŸºç¡€ç»Ÿè®¡
    total_chunks = len(processed_chunks)
    total_chars = sum(len(chunk.content) for chunk in processed_chunks)
    avg_chunk_size = total_chars / total_chunks if total_chunks > 0 else 0

    # æºåˆ†å¸ƒ
    source_dist = {}
    for chunk in processed_chunks:
        source = chunk.metadata["source"]
        source_dist[source] = source_dist.get(source, 0) + 1

    print(f"ğŸ“„ å¤„ç†æ–‡æ¡£å—: {total_chunks}")
    print(f"ğŸ“Š æ€»å­—ç¬¦æ•°: {total_chars:,}")
    print(f"ğŸ“ å¹³å‡å—å¤§å°: {avg_chunk_size:.0f} å­—ç¬¦")
    print(f"â±ï¸ æ€»å¤„ç†æ—¶é—´: {total_time:.2f} ç§’")

    print(f"\nğŸ“‹ æ•°æ®æºåˆ†å¸ƒ:")
    for source, count in source_dist.items():
        percentage = count / total_chunks * 100
        print(f"  {source}: {count} ({percentage:.1f}%)")

    print(f"\nğŸ—ï¸ ç´¢å¼•æ„å»ºçŠ¶æ€:")
    qdrant_status = "âœ… æˆåŠŸ" if qdrant_success else "âŒ å¤±è´¥"
    es_status = "âœ… æˆåŠŸ" if es_success else "âŒ å¤±è´¥"
    print(f"  Qdrantå‘é‡ç´¢å¼•: {qdrant_status}")
    print(f"  Elasticsearchå…¨æ–‡ç´¢å¼•: {es_status}")

    if qdrant_success and es_success:
        print(f"\nğŸ‰ åŒé‡ç´¢å¼•æ„å»ºå®Œæˆï¼RAGç³»ç»Ÿå·²å‡†å¤‡å°±ç»ªã€‚")
    elif qdrant_success or es_success:
        print(f"\nâš ï¸ éƒ¨åˆ†ç´¢å¼•æ„å»ºæˆåŠŸï¼Œè¯·æ£€æŸ¥å¤±è´¥çš„æœåŠ¡ã€‚")
    else:
        print(f"\nâŒ ç´¢å¼•æ„å»ºå¤±è´¥ï¼Œè¯·æ£€æŸ¥æœåŠ¡é…ç½®ã€‚")

    print("=" * 60)


def main():
    """ä¸»å‡½æ•°"""
    start_time = time.time()

    print("ğŸš€ Smart RAG MVP - ä¸€ä½“åŒ–ç´¢å¼•æ„å»ºï¼ˆQdrant 1.7å…¼å®¹ç‰ˆï¼‰")
    print("=" * 60)

    # æ­¥éª¤1: è®¾ç½®Gemini API
    if not setup_gemini_api():
        exit(1)

    processed_chunks_file = Path("data/processed_chunks.json")
    processed_chunks = []

    if processed_chunks_file.exists():
        print(f"âœ… å‘ç°å·²å­˜åœ¨çš„å¤„ç†æ–‡ä»¶: {processed_chunks_file}ï¼Œè·³è¿‡åµŒå…¥ç”Ÿæˆæ­¥éª¤ã€‚")
        with open(processed_chunks_file, 'r', encoding='utf-8') as f:
            loaded_data = json.load(f)

        # ä»åŠ è½½çš„æ•°æ®é‡å»ºProcessedChunkå¯¹è±¡ï¼Œå¹¶ä¿®æ­£æ—¶é—´æ ¼å¼
        for chunk_data in loaded_data:
            # ä¿®æ­£æ—§æ–‡ä»¶ä¸­çš„æ—¶é—´æˆ³æ ¼å¼
            ts = chunk_data["metadata"]["created_at"]
            if isinstance(ts, (int, float)):
                chunk_data["metadata"]["created_at"] = datetime.fromtimestamp(ts).isoformat()

            processed_chunks.append(ProcessedChunk(**chunk_data))
        print(f"ğŸ”§ å·²ä»æ–‡ä»¶åŠ è½½å¹¶ä¿®æ­£ {len(processed_chunks)} ä¸ªå¤„ç†å—ã€‚")

    else:
        print("â„¹ï¸ æœªå‘ç°å·²å¤„ç†æ–‡ä»¶ï¼Œå°†æ‰§è¡Œå®Œæ•´å¤„ç†æµç¨‹ã€‚")
        # æ­¥éª¤2: åŠ è½½æ–‡æ¡£
        documents = load_documents()
        if not documents:
            print("âŒ æ²¡æœ‰æ–‡æ¡£å¯å¤„ç†")
            exit(1)

        # æ­¥éª¤3: æ–‡æ¡£åˆ†å‰²
        chunks = split_documents(documents)
        if not chunks:
            print("âŒ æ–‡æ¡£åˆ†å‰²å¤±è´¥")
            exit(1)

        # æ­¥éª¤4: ç”ŸæˆåµŒå…¥
        embeddings = generate_embeddings(chunks)
        if len(embeddings) != len(chunks):
            print("âŒ åµŒå…¥ç”Ÿæˆæ•°é‡ä¸åŒ¹é…")
            exit(1)

        # æ­¥éª¤5: ç»„åˆå¤„ç†ç»“æœ
        processed_chunks = create_processed_chunks(chunks, embeddings)

    # æ­¥éª¤6: è®¾ç½®æ•°æ®åº“
    qdrant_client = setup_qdrant_v17()
    es_client = setup_elasticsearch_fixed()

    # æ­¥éª¤7: æ„å»ºç´¢å¼•
    qdrant_success = build_qdrant_index_v17(qdrant_client, processed_chunks)
    es_success = build_elasticsearch_index_fixed(es_client, processed_chunks)

    # æ­¥éª¤8: ä¿å­˜ç»“æœ
    save_processing_results(processed_chunks)

    # æ­¥éª¤9: æ˜¾ç¤ºæ‘˜è¦
    total_time = time.time() - start_time
    print_summary(processed_chunks, qdrant_success, es_success, total_time)


if __name__ == "__main__":
    main()